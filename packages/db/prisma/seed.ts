import { PrismaClient } from "@prisma/client";

const prisma = new PrismaClient();

const models = [
  // Llama 3.1 Series
  {
    id: "llama-3.1-8b",
    hfId: "meta-llama/Meta-Llama-3.1-8B-Instruct",
    ollamaModelId: "llama3.1:8b",
    displayName: "Llama 3.1 8B Instruct",
    description:
      "Meta's latest instruction-tuned model. Excellent for general tasks.",
    parameters: BigInt(8_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "llama3.1",
    tags: ["chat", "instruct", "general"],
    pricingTier: "small",
  },
  {
    id: "llama-3.1-70b",
    hfId: "meta-llama/Meta-Llama-3.1-70B-Instruct",
    ollamaModelId: "llama3.1:70b",
    displayName: "Llama 3.1 70B Instruct",
    description:
      "Larger Llama model with superior reasoning and instruction following.",
    parameters: BigInt(70_000_000_000),
    minGpuTier: "a100",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "llama3.1",
    tags: ["chat", "instruct", "reasoning"],
    pricingTier: "large",
  },
  {
    id: "llama-3.2-3b",
    hfId: "meta-llama/Llama-3.2-3B-Instruct",
    ollamaModelId: "llama3.2:3b",
    displayName: "Llama 3.2 3B Instruct",
    description: "Compact Llama model optimized for efficiency.",
    parameters: BigInt(3_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 8192,
    license: "llama3.2",
    tags: ["chat", "compact", "efficient"],
    pricingTier: "small",
  },

  // Mistral Series
  {
    id: "mistral-7b",
    hfId: "mistralai/Mistral-7B-Instruct-v0.2",
    ollamaModelId: "mistral:7b",
    displayName: "Mistral 7B Instruct",
    description: "Fast and efficient model from Mistral AI.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "apache-2.0",
    tags: ["chat", "instruct", "fast"],
    pricingTier: "small",
  },
  {
    id: "mixtral-8x7b",
    hfId: "mistralai/Mixtral-8x7B-Instruct-v0.1",
    ollamaModelId: "mixtral:8x7b",
    displayName: "Mixtral 8x7B",
    description: "Mixture of Experts model with excellent performance.",
    parameters: BigInt(46_700_000_000),
    minGpuTier: "a100",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 32768,
    license: "apache-2.0",
    tags: ["chat", "moe", "high-performance"],
    pricingTier: "large",
  },

  // Code Models
  {
    id: "codellama-7b",
    hfId: "codellama/CodeLlama-7b-Instruct-hf",
    ollamaModelId: "codellama:7b",
    displayName: "Code Llama 7B",
    description: "Fast code generation model for everyday tasks.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 16384,
    license: "llama2",
    tags: ["code", "instruct", "programming"],
    pricingTier: "small",
  },
  {
    id: "codellama-34b",
    hfId: "codellama/CodeLlama-34b-Instruct-hf",
    ollamaModelId: "codellama:34b",
    displayName: "Code Llama 34B",
    description: "Specialized model for code generation and understanding.",
    parameters: BigInt(34_000_000_000),
    minGpuTier: "4090",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 16384,
    license: "llama2",
    tags: ["code", "instruct", "programming"],
    pricingTier: "medium",
  },
  {
    id: "deepseek-coder-6.7b",
    hfId: "deepseek-ai/deepseek-coder-6.7b-instruct",
    ollamaModelId: "deepseek-coder:6.7b",
    displayName: "DeepSeek Coder 6.7B",
    description: "Efficient code generation model from DeepSeek.",
    parameters: BigInt(6_700_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 16384,
    license: "deepseek",
    tags: ["code", "instruct", "programming"],
    pricingTier: "small",
  },
  {
    id: "deepseek-coder-33b",
    hfId: "deepseek-ai/deepseek-coder-33b-instruct",
    ollamaModelId: "deepseek-coder:33b",
    displayName: "DeepSeek Coder 33B",
    description: "Excellent code generation model from DeepSeek.",
    parameters: BigInt(33_000_000_000),
    minGpuTier: "4090",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 16384,
    license: "deepseek",
    tags: ["code", "instruct", "programming"],
    pricingTier: "medium",
  },

  // Qwen Series
  {
    id: "qwen2-7b",
    hfId: "Qwen/Qwen2-7B-Instruct",
    ollamaModelId: "qwen2:7b",
    displayName: "Qwen2 7B Instruct",
    description: "Alibaba's multilingual model with strong performance.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 32768,
    license: "apache-2.0",
    tags: ["chat", "multilingual", "long-context"],
    pricingTier: "small",
  },
  {
    id: "qwen2.5-coder-7b",
    hfId: "Qwen/Qwen2.5-Coder-7B-Instruct",
    ollamaModelId: "qwen2.5-coder:7b",
    displayName: "Qwen2.5 Coder 7B",
    description: "Specialized coding model from Alibaba.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 32768,
    license: "apache-2.0",
    tags: ["code", "multilingual", "programming"],
    pricingTier: "small",
  },

  // Phi Series (Microsoft)
  {
    id: "phi-3-mini",
    hfId: "microsoft/Phi-3-mini-4k-instruct",
    ollamaModelId: "phi3:mini",
    displayName: "Phi-3 Mini 4K",
    description: "Microsoft's compact but capable model.",
    parameters: BigInt(3_800_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 4096,
    license: "mit",
    tags: ["chat", "compact", "efficient"],
    pricingTier: "small",
  },
  {
    id: "phi-3-medium",
    hfId: "microsoft/Phi-3-medium-4k-instruct",
    ollamaModelId: "phi3:medium",
    displayName: "Phi-3 Medium 4K",
    description: "Larger Phi-3 model with enhanced capabilities.",
    parameters: BigInt(14_000_000_000),
    minGpuTier: "4070",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 4096,
    license: "mit",
    tags: ["chat", "reasoning", "efficient"],
    pricingTier: "medium",
  },

  // Gemma Series (Google)
  {
    id: "gemma-2b",
    hfId: "google/gemma-2b-it",
    ollamaModelId: "gemma:2b",
    displayName: "Gemma 2B Instruct",
    description: "Google's lightweight open model.",
    parameters: BigInt(2_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 8192,
    license: "gemma",
    tags: ["chat", "compact", "efficient"],
    pricingTier: "small",
  },
  {
    id: "gemma-7b",
    hfId: "google/gemma-7b-it",
    ollamaModelId: "gemma:7b",
    displayName: "Gemma 7B Instruct",
    description: "Google's capable 7B open model.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "gemma",
    tags: ["chat", "instruct", "general"],
    pricingTier: "small",
  },
  {
    id: "gemma2-9b",
    hfId: "google/gemma-2-9b-it",
    ollamaModelId: "gemma2:9b",
    displayName: "Gemma 2 9B Instruct",
    description: "Google's latest Gemma 2 model with improved performance.",
    parameters: BigInt(9_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "gemma",
    tags: ["chat", "instruct", "reasoning"],
    pricingTier: "small",
  },

  // Specialized Models
  {
    id: "neural-chat-7b",
    hfId: "Intel/neural-chat-7b-v3-1",
    ollamaModelId: "neural-chat:7b",
    displayName: "Neural Chat 7B",
    description: "Intel's optimized chat model.",
    parameters: BigInt(7_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 8192,
    license: "apache-2.0",
    tags: ["chat", "instruct", "optimized"],
    pricingTier: "small",
  },
  {
    id: "stablelm-2-zephyr",
    hfId: "stabilityai/stablelm-2-zephyr-1_6b",
    ollamaModelId: "stablelm2:1.6b",
    displayName: "StableLM 2 Zephyr 1.6B",
    description: "Stability AI's ultra-compact chat model.",
    parameters: BigInt(1_600_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 4096,
    license: "stability-ai-community",
    tags: ["chat", "compact", "fast"],
    pricingTier: "small",
  },
  {
    id: "yi-6b",
    hfId: "01-ai/Yi-6B-Chat",
    ollamaModelId: "yi:6b",
    displayName: "Yi 6B Chat",
    description: "01.AI's efficient chat model with strong Chinese support.",
    parameters: BigInt(6_000_000_000),
    minGpuTier: "3060",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "ollama",
    contextLength: 4096,
    license: "yi",
    tags: ["chat", "multilingual", "chinese"],
    pricingTier: "small",
  },
  {
    id: "yi-34b",
    hfId: "01-ai/Yi-34B-Chat",
    ollamaModelId: "yi:34b",
    displayName: "Yi 34B Chat",
    description: "01.AI's large chat model with excellent capabilities.",
    parameters: BigInt(34_000_000_000),
    minGpuTier: "4090",
    supportedEngines: ["vllm", "ollama"],
    defaultEngine: "vllm",
    contextLength: 4096,
    license: "yi",
    tags: ["chat", "multilingual", "reasoning"],
    pricingTier: "medium",
  },

  // vLLM-only Large Models
  {
    id: "llama-3.1-405b",
    hfId: "meta-llama/Meta-Llama-3.1-405B-Instruct",
    ollamaModelId: null,
    displayName: "Llama 3.1 405B Instruct",
    description: "Meta's flagship model with unmatched capabilities.",
    parameters: BigInt(405_000_000_000),
    minGpuTier: "h100",
    supportedEngines: ["vllm"],
    defaultEngine: "vllm",
    contextLength: 8192,
    license: "llama3.1",
    tags: ["chat", "instruct", "flagship"],
    pricingTier: "enterprise",
  },
];

async function main() {
  console.log("Seeding database...");

  // Upsert models
  for (const model of models) {
    await prisma.model.upsert({
      where: { id: model.id },
      update: model,
      create: model,
    });
    console.log(`  Seeded model: ${model.id}`);
  }

  console.log("Seeding complete!");
}

main()
  .catch((e) => {
    console.error(e);
    process.exit(1);
  })
  .finally(async () => {
    await prisma.$disconnect();
  });
